{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7- Data Analysis\n",
    "\n",
    "Exercises 1-4 are to be completed by Match 29th. The remaider of the lab is due April 5th. Before leaving lab today, everyone must download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Reading\n",
    "\n",
    "### HiggsML\n",
    "In 2014, some of my colleagues from the ATLAS experiment put together a Higgs Machine Learning Challenge, which was hosted on [Kaggle](https://www.kaggle.com). Please read sections 1 and 3 (skip/skim 2) of [The HiggsML Technical Documentation](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf). \n",
    "\n",
    "Kaggle is a platform for data science competitions, with cash awards for winners. Kaggle currently hosts over 50,000 public datasets and associated competitions. Later in the course we will look at a variety of problems hosted on Kaggle and similar platforms. \n",
    "\n",
    "### SUSY Dataset\n",
    "\n",
    "For the next few labs we will use datasets used in the [first paper on Deep Learning in High Energy physics](https://arxiv.org/pdf/1402.4735.pdf). Please read up to the \"Deep Learning\" section (end of page 5). This paper demonstrates that Deep Neural Networks can learn from raw data the features that are typically used by physicists for searches for exotics particles. The authors provide the data they used for this paper. They considered two benchmark scenarios: Higgs and SUSY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Download SUSY Dataset\n",
    "\n",
    "The information about the dataset can be found at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We'll start with the [SUSY Dataset](https://archive.ics.uci.edu/ml/datasets/SUSY). \n",
    "\n",
    "### Download\n",
    "In a terminal, download the data directly from the source and then decompress it. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To download:\n",
    "    * On Mac OS: \n",
    "    `curl http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz > SUSY.csv.gz`\n",
    "\n",
    "    * In linux:\n",
    "    `wget http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz`\n",
    "\n",
    "* To uncompress:\n",
    "`gunzip SUSY.csv.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-02 19:07:18--  http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘SUSY.csv.gz.5’\n",
      "\n",
      "SUSY.csv.gz.5           [            <=>     ] 879.65M  6.39MB/s    in 2m 9s   \n",
      "\n",
      "2024-05-02 19:09:27 (6.83 MB/s) - ‘SUSY.csv.gz.5’ saved [922377711]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: SUSY.csv already exists; do you wish to overwrite (y or n)? "
     ]
    }
   ],
   "source": [
    "!gunzip SUSY.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is provided as a comma separated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"SUSY.csv\"\n",
    "# print out the first 5 lines using unix head command\n",
    "!head -5  \"SUSY.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Look\n",
    "\n",
    "Each row represents a LHC collision event. Each column contains some observable from that event. The variable names are ([based on documentation](https://archive.ics.uci.edu/ml/datasets/SUSY)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarNames=[\"signal\", \"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these variables represent the \"raw\" kinematics of the observed final state particles, while others are \"features\" that are derived from these raw quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawNames=[\"l_1_pT\", \"l_1_eta\",\"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\"]\n",
    "FeatureNames=list(set(VarNames[1:]).difference(RawNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pandas to read in the file, and matplotlib to make plots. The following ensures pandas is installed and sets everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"SUSY.csv\"\n",
    "df = pd.read_csv(filename, dtype='float64', names=VarNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the data in Jupyter by just evaluateing the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column stores the \"truth\" label of whether an event was signal or not. Pandas makes it easy to create dataframes that store only the signal or background events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig=df[df.signal==1]\n",
    "df_bkg=df[df.signal==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example plots the signal and background distributions of every variable. Note that we use VarNames[1:] to skip the first variable, which was the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for var in VarNames[1:]:\n",
    "    print (var)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(np.array(df_sig[var]),bins=100,histtype=\"step\", color=\"red\",label=\"signal\",density=1, stacked=True)\n",
    "    plt.hist(np.array(df_bkg[var]),bins=100,histtype=\"step\", color=\"blue\", label=\"background\",density=1, stacked=True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Make nice figures\n",
    "\n",
    "Now use `matplotlib` to reproduce as closely as you can figures 5 and 6 from the paper. This exercise is intended to get you to familiarize yourself with making nicely formatted `matplotlib` figures with multiple plots. Note that the plots in the paper are actually wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define variables and features\n",
    "VarNames = [\"signal\", \"l_1_pT\", \"l_1_eta\", \"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \n",
    "            \"MET\", \"MET_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \n",
    "            \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos_theta_r1\"]\n",
    "RawNames = [\"l_1_pT\", \"l_1_eta\", \"l_1_phi\", \"l_2_pT\", \"l_2_eta\", \"l_2_phi\", \"MET\", \"MET_phi\"]\n",
    "FeatureNames = list(set(VarNames[1:]).difference(RawNames))\n",
    "\n",
    "# Filter data into signal and background\n",
    "df_sig = df[df.signal == 1]\n",
    "df_bkg = df[df.signal == 0]\n",
    "\n",
    "# Create subplots for figures 5 and 6\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 12))\n",
    "\n",
    "# Plot histograms for each variable\n",
    "for ax, var in zip(axes, FeatureNames):\n",
    "    ax.hist(df_sig[var], bins=100, histtype=\"step\", color=\"red\", label=\"signal\", density=True)\n",
    "    ax.hist(df_bkg[var], bins=100, histtype=\"step\", color=\"blue\", label=\"background\", density=True)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Normalized Count')\n",
    "\n",
    "# Add titles and adjust layout\n",
    "axes[0].set_title('Figure 5: Distributions of Derived Features (Signal vs. Background)')\n",
    "axes[1].set_title('Figure 6: Distributions of Derived Features (Signal vs. Background)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Correlation\n",
    "\n",
    "### Exercise 4.1\n",
    "\n",
    "#### Part a\n",
    "Write a function that creates pair plots and use it to compare variables in the SUSY and Higgs samples, separately for low and high-level features. Refer to Lecture 13 for details. Do not use `seaborn`.\n",
    "\n",
    "#### Part b\n",
    "Making these plots can be slow because creating each plot initiates a full loop over the data. Make at least one modification to your function in part a to speed it up. Can you propose a different method of creating histograms that would speed up making such pair plots?\n",
    "\n",
    "#### Part c\n",
    "Which observables appear to be best for separating signal from background?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "\n",
    "columns = df.columns[1:6]\n",
    "n_columns=len(columns)\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plot_i=0\n",
    "for i,x_var_name in enumerate(columns):\n",
    "    for j,y_var_name in enumerate(columns):\n",
    "        plot_i+=1\n",
    "        plt.subplot(n_columns,n_columns,plot_i)\n",
    "        make_legend = plot_i==1\n",
    "        if i==j:\n",
    "            compare_distributions(df,x_var_name,\n",
    "                     selection_dict,\n",
    "                     alpha=0.5,\n",
    "                     density=1,\n",
    "                     bins=50,\n",
    "                     )\n",
    "        else:\n",
    "            compare_scatter(df,x_var_name,y_var_name,selection_dict,make_legend=make_legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    "\n",
    "#### Part a\n",
    "Install [tabulate](https://github.com/astanin/python-tabulate). \n",
    "\n",
    "#### Part b\n",
    "Use numpy to compute the [covariance matrix](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) and [correlation matrix](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) between all observabes, and separately between low and high-level features.\n",
    "\n",
    "#### Part c\n",
    "Use tabulate to create a well formatted table of the covariance and correlation matrices, with nice headings and appropriate significant figures. Embed the table into this notebook.\n",
    "\n",
    "#### Part d\n",
    "Write a function that takes a dataset and appropriate arguments and performs steps b and c.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "def compute_and_display_matrices(dataset, raw_names, feature_names):\n",
    "    # Compute covariance matrix for all observables\n",
    "    cov_all = np.cov(dataset[:, 1:].T)\n",
    "    \n",
    "    # Compute correlation matrix for all observables\n",
    "    corr_all = np.corrcoef(dataset[:, 1:].T)\n",
    "    \n",
    "    # Compute covariance matrix for low-level features\n",
    "    cov_low = np.cov(dataset[:, 1+len(feature_names):1+len(feature_names)+len(raw_names)].T)\n",
    "    \n",
    "    # Compute correlation matrix for low-level features\n",
    "    corr_low = np.corrcoef(dataset[:, 1+len(feature_names):1+len(feature_names)+len(raw_names)].T)\n",
    "    \n",
    "    # Create headers\n",
    "    headers = [''] + ['Var ' + str(i) for i in range(1, len(raw_names) + 1)]\n",
    "    \n",
    "    # Create rows for covariance matrix\n",
    "    cov_rows = [['Var ' + str(i)] + list(row) for i, row in enumerate(cov_all, start=1)]\n",
    "    \n",
    "    # Create rows for correlation matrix\n",
    "    corr_rows = [['Var ' + str(i)] + list(row) for i, row in enumerate(corr_all, start=1)]\n",
    "    \n",
    "    # Create rows for low-level covariance matrix\n",
    "    low_cov_rows = [['Var ' + str(i)] + list(row) for i, row in enumerate(cov_low, start=1)]\n",
    "    \n",
    "    # Create rows for low-level correlation matrix\n",
    "    low_corr_rows = [['Var ' + str(i)] + list(row) for i, row in enumerate(corr_low, start=1)]\n",
    "    \n",
    "    # Format matrices into tables\n",
    "    cov_table = tabulate(cov_rows, headers=headers, tablefmt='fancy_grid', floatfmt=\".2f\")\n",
    "    corr_table = tabulate(corr_rows, headers=headers, tablefmt='fancy_grid', floatfmt=\".2f\")\n",
    "    low_cov_table = tabulate(low_cov_rows, headers=headers, tablefmt='fancy_grid', floatfmt=\".2f\")\n",
    "    low_corr_table = tabulate(low_corr_rows, headers=headers, tablefmt='fancy_grid', floatfmt=\".2f\")\n",
    "    \n",
    "    # Display tables\n",
    "    print(\"Covariance Matrix (All Observables):\\n\", cov_table)\n",
    "    print(\"\\nCorrelation Matrix (All Observables):\\n\", corr_table)\n",
    "    print(\"\\nCovariance Matrix (Low-level Features):\\n\", low_cov_table)\n",
    "    print(\"\\nCorrelation Matrix (Low-level Features):\\n\", low_corr_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Example code for embedding a `tabulate` table into a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [[\"A\",1,2],\n",
    "        [\"C\",3,4],\n",
    "        [\"D\",5,6]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"X\",\"Y\",\"Z\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Selection\n",
    "\n",
    "### Exercise 5.1\n",
    "\n",
    "Part a\n",
    "By looking at the signal/background distributions for each observable (e.g. $x$) determine which selection criteria would be optimal: \n",
    "\n",
    "1. $x > x_c$\n",
    "2. $x < x_c$\n",
    "3. $|x - \\mu| > x_c$\n",
    "4. $|x - \\mu| < x_c$\n",
    "\n",
    "where $x_c$ is value to be determined below.\n",
    "\n",
    "### Exercise 5.2\n",
    "\n",
    "Plot the True Positive Rate (TPR) (aka signal efficiency $\\epsilon_S(x_c)$) and False Positive Rate (FPR) (aka background efficiency $\\epsilon_B(x_c)$) as function of $x_c$ for applying the strategy in part a to each observable. \n",
    "\n",
    "### Exercise 5.3\n",
    "Assume 3 different scenarios corresponding to different numbers of signal and background events expected in data:\n",
    "\n",
    "1. Expect $N_S=10$, $N_B=100$.\n",
    "1. Expect $N_S=100$, $N_B=1000$.\n",
    "1. Expect $N_S=1000$, $N_B=10000$.\n",
    "1. Expect $N_S=10000$, $N_B=100000$.\n",
    "\n",
    "Plot the significance ($\\sigma_{S'}$) for each observable as function of $x_c$ for each scenario, where \n",
    "\n",
    "$\\sigma_{S'}= \\frac{N'_S}{\\sqrt{N'_S+N'_B}}$\n",
    "\n",
    "and $N'_{S,B} = \\epsilon_{S,B}(x_c) * N_{S,B}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data for signal and background distributions\n",
    "np.random.seed(0)\n",
    "df_sig = df[df.signal == 1]\n",
    "df_bkg = df[df.signal == 0]\n",
    "signal_data = np.random.normal(loc=5, scale=2, size=1000)\n",
    "background_data = np.random.normal(loc=0, scale=2, size=1000)\n",
    "\n",
    "\n",
    "# Define the observable x\n",
    "x = np.linspace(-5, 15, 100)\n",
    "\n",
    "# Exercise 5.1: Determine optimal selection criteria\n",
    "# Plot signal and background distributions\n",
    "plt.hist(signal_data, bins=30, alpha=0.5, color='b', label='Signal')\n",
    "plt.hist(background_data, bins=30, alpha=0.5, color='r', label='Background')\n",
    "plt.xlabel('Observable (x)')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Signal vs Background Distributions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Cut Flow\n",
    "\n",
    "\n",
    "### Exercise 6.1\n",
    "\n",
    "For each above scenario, choose a subset (minumum 3) of observables to use for selections, and values of $x_c$ based on your significance plots (part 3c). \n",
    "\n",
    "### Exercise 6.2\n",
    "Create a \"cut-flow\" table for each scenario where you successively make the selections on each observable and tabulate $\\epsilon_S$, $\\epsilon_B$, $N'_S$, $N'_B$, and $\\sigma_{S'}$.\n",
    "\n",
    "### Exercise 6.3\n",
    "In 3c above you computed the significance for each observable assuming to make no other selections on any other observable. If the variables are correlated, then this assumption can lead to non-optimial results when selecting on multiple variables. By looking at the correlation matrices and your answers to 4b, identify where this effect could be most detrimental to the significance. Attempt to correct the issue by applying the selection in one observable and then optimizing (part 3c) for a second observable. What happens if you change the order of your selection (make selection on second and optimize on first)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: ROC Curves\n",
    "\n",
    "### Exercise 7.1\n",
    "For the top 3 observables you identified earlier, create one figure overlaying the Reciever Operating Characteristic (ROC) curves for the 3 observables. Compute the area under the curves and report it in the legend of the figure.\n",
    "\n",
    "### Exercise 7.2\n",
    "Write a function that you can use to quickly create the figure in part a with other observables and different conditions. Note that you will likely revise this function as you do the remainder of the lab.\n",
    "\n",
    "### Exercise 7.3\n",
    "Use the function from part b to compare the ROC curves for the successive selections in lab 3, exercise 4. Specifically, plot the ROC curve after each selection.\n",
    "\n",
    "### Exercise 7.4\n",
    "Use your function and appropriate example to demonstrate the effect (if any) of changing order of the successive selections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define functions to compute epsilon_S, epsilon_B, N'_S, N'_B, and sigma_S'\n",
    "def epsilon_s(epsilon_b, n_s, n_b):\n",
    "    return (epsilon_s * n_s) / (epsilon_s * n_s + epsilon_b * n_b)\n",
    "\n",
    "def epsilon_b(epsilon_s, n_s, n_b):\n",
    "    return (epsilon_b * n_b) / (epsilon_s * n_s + epsilon_b * n_b)\n",
    "\n",
    "def n_prime_s(epsilon_s, n_s):\n",
    "    return epsilon_s * n_s\n",
    "\n",
    "def n_prime_b(epsilon_b, n_b):\n",
    "    return epsilon_b * n_b\n",
    "\n",
    "def sigma_s_prime(n_prime_s, n_prime_b):\n",
    "    return n_prime_s / np.sqrt(n_prime_s + n_prime_b)\n",
    "\n",
    "# Define a function to create the cut-flow table\n",
    "def cut_flow_table(scenario, observables, x_c_values):\n",
    "    results = pd.DataFrame(columns=['Observable', 'epsilon_S', 'epsilon_B', 'N_prime_S', 'N_prime_B', 'sigma_S_prime'])\n",
    "    for i, observable in enumerate(observables):\n",
    "        # Apply selection criteria for the observable based on x_c_values[i]\n",
    "        epsilon_s_val = compute_epsilon_s(x_c_values[i], scenario)\n",
    "        epsilon_b_val = compute_epsilon_b(x_c_values[i], scenario)\n",
    "        n_prime_s_val = compute_n_prime_s(epsilon_s_val, scenario)\n",
    "        n_prime_b_val = compute_n_prime_b(epsilon_b_val, scenario)\n",
    "        sigma_s_prime_val = compute_sigma_s_prime(n_prime_s_val, n_prime_b_val)\n",
    "        results.loc[i] = [observable, epsilon_s_val, epsilon_b_val, n_prime_s_val, n_prime_b_val, sigma_s_prime_val]\n",
    "    return results\n",
    "\n",
    "# Define the scenarios\n",
    "scenarios = {\n",
    "    'Scenario 1': {'N_S': 10, 'N_B': 100},\n",
    "    'Scenario 2': {'N_S': 100, 'N_B': 1000},\n",
    "    'Scenario 3': {'N_S': 1000, 'N_B': 10000},\n",
    "    'Scenario 4': {'N_S': 10000, 'N_B': 100000}\n",
    "}\n",
    "\n",
    "# Define the observables and x_c values for each scenario\n",
    "observables = {\n",
    "    'Scenario 1': ['Observable 1', 'Observable 2', 'Observable 3'],\n",
    "    'Scenario 2': ['Observable 1', 'Observable 2', 'Observable 3'],\n",
    "    'Scenario 3': ['Observable 1', 'Observable 2', 'Observable 3'],\n",
    "    'Scenario 4': ['Observable 1', 'Observable 2', 'Observable 3']\n",
    "}\n",
    "\n",
    "x_c_values = {\n",
    "    'Scenario 1': [0.5, 0.7, 0.9],\n",
    "    'Scenario 2': [0.4, 0.6, 0.8],\n",
    "    'Scenario 3': [0.3, 0.5, 0.7],\n",
    "    'Scenario 4': [0.2, 0.4, 0.6]\n",
    "}\n",
    "\n",
    "# Create cut-flow tables for each scenario\n",
    "for scenario_name, scenario in scenarios.items():\n",
    "    print(f\"Cut-flow table for {scenario_name}:\")\n",
    "    print(cut_flow_table(scenario, observables[scenario_name], x_c_values[scenario_name]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Linear Discriminant\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Using numpy, compute the between-class $\\bf{S}_B$ and within-class $\\bf{S}_W$ covariance matrices defined as:\n",
    "\n",
    "$$\n",
    "\\bf{S}_B = (\\bf{m_2}-\\bf{m_1})(\\bf{m_2}-\\bf{m_1})^T \\\\\n",
    "$$\n",
    "$$\n",
    "\\bf{S}_W = \\sum_{i=1,2} \\sum_{n=1}^{l_i} (\\bf{x}_n^i - \\bf{m}_i) (\\bf{x}_n^i - \\bf{m}_i)^T\n",
    "$$\n",
    "\n",
    "where $\\bf{m_i}$ are the vectors containing the means for category 1 and 2, here defined as signal and background. Here $\\bf{x}_n^i$ is the vector containing the observables for the $n$th example event in category $i$.\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Compute the linear coefficients $\\bf{w} = \\bf{S_W}^{-1}(\\bf{m_2}-\\bf{m_1})$. Compare the histogram of the distribution of $F_n^i=\\bf{w}^T\\bf{x}_n^i$ for the two categories.\n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "Draw the ROC curve for $F_n$. \n",
    "\n",
    "### Exercise 8.1\n",
    "\n",
    "What is the maximal significance you can obtain in the scenarios in exercise 5? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the means for category 1 and 2 (signal and background)\n",
    "m1 = np.mean(category1_data, axis=0)\n",
    "m2 = np.mean(category2_data, axis=0)\n",
    "\n",
    "# Compute the between-class covariance matrix S_B\n",
    "S_B = np.outer((m2 - m1), (m2 - m1))\n",
    "\n",
    "# Compute the within-class covariance matrix S_W\n",
    "S_W = np.zeros_like(S_B)\n",
    "for i, category_data in enumerate([category1_data, category2_data]):\n",
    "    for x in category_data:\n",
    "        x_minus_m = x - (m1 if i == 0 else m2)\n",
    "        S_W += np.outer(x_minus_m, x_minus_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the linear coefficients w\n",
    "S_W_inv = np.linalg.inv(S_W)\n",
    "w = np.dot(S_W_inv, (m2 - m1))\n",
    "\n",
    "# Compute the linear discriminant values for each category\n",
    "F_category1 = np.dot(category1_data, w)\n",
    "F_category2 = np.dot(category2_data, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_significance(category1_scores, category2_scores, threshold):\n",
    "    num_signal = np.sum(category1_scores >= threshold)\n",
    "    num_background = np.sum(category2_scores >= threshold)\n",
    "    return num_signal / np.sqrt(num_signal + num_background)\n",
    "\n",
    "# Compute the significance for various threshold values\n",
    "thresholds = np.linspace(np.min(F_category1), np.max(F_category2), 100)\n",
    "significances = [compute_significance(F_category1, F_category2, threshold) for threshold in thresholds]\n",
    "\n",
    "# Find the maximum significance\n",
    "max_significance = np.max(significances)\n",
    "max_threshold = thresholds[np.argmax(significances)]\n",
    "print(f\"Maximal significance: {max_significance} at threshold: {max_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
